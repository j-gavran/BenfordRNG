{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018f990-8929-4ce0-9778-3a4adae2836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, os.getcwd() + '/reddit_download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa464c-6fcd-47ed-bf9c-f5539279ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append('../..')\n",
    "from plotting.matplotlib_setup import configure_latex, savefig, set_size_decorator, savefig, thiner_border\n",
    "\n",
    "tex_dir, images_dir = 'porocilo/main.tex', 'porocilo/images'\n",
    "\n",
    "configure_latex(style=['science', 'notebook'], global_save_path=images_dir)\n",
    "\n",
    "%config InlineBackend.figure_format = 'pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b85d3cc-0754-4678-89fe-ca454e3dd769",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3420eb-ea97-4881-9efd-f1cd6a01368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments = pd.read_csv('comments.csv', lineterminator='\\n')\n",
    "df_posts = pd.read_csv('posts.csv', lineterminator='\\n')\n",
    "\n",
    "df_comments.drop(columns=['author', 'post_id', 'parent_id', 'permalink'], inplace=True)\n",
    "df_posts.drop(columns=['author', 'post_id', 'num_comments', 'permalink'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb9c181-8002-4caa-8a2b-b99518438b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments['body'] = df_comments['body'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b8d54-e2d4-49f7-a1bf-d8b8d772cd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comments.sort_values(by=['timestamp'], inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b43cd0f-6ba5-423c-85f0-d6afe1c8ffe4",
   "metadata": {},
   "source": [
    "# RNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e7311c-6d87-4000-acba-612cd0e422cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benford_helper_functions import str_to_bits, get_bitstring, binary_tree_walk\n",
    "from random_helper_functions import split_to_arr, bin_str_to_matrix\n",
    "from NIST_tests import RNG_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c5fb0-08b3-4c9f-9557-8b2e1de3e30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r/Genshin_Impact leaking much\n",
    "top_1000_words = ['the', 'to', 'I', 'a', 'and', 'of', 'is', 'in', 'that', 'you', 'it', 'for', 'was', 'my', 'with', 'on', 'but', 'have', 'be', 'not', 'are', 'just', 'like', 'as', 'or', 'so', 'they', 'this', 'at', 'if', 'me', 'can', 'your', 'The', 'get', 'about', 'from', 'would', 'all', 'one', 'do', 'an', 'people', 'when', 'up', 'out', 'more', 'what', 'her', 'because', \"don't\", \"I'm\", 'we', 'had', 'he', 'i', 'some', 'think', 'will', \"it's\", 'by', 'them', 'really', 'their', 'how', 'has', 'no', 'only', 'know', 'who', 'even', 'than', 'good', 'there', 'time', 'she', 'his', 'then', 'It', 'other', 'If', 'got', 'want', 'You', 'still', 'much', 'were', 'make', 'been', 'also', 'being', 'it.', 'go', 'into', 'any', 'could', 'see', 'never', 'My', 'very', 'But', 'And', 'need', 'way', 'use', \"It's\", 'which', '-', 'most', 'first', 'going', 'him', 'after', 'something', 'where', 'This', 'too', 'I’m', 'say', 'same', 'should', 'lot', 'back', 'over', 'did', 'better', 'A', 'actually', 'now', 'every', 'So', 'pretty', 'always', 'why', 'don’t', 'someone', 'They', 'off', 'those', 'it’s', 'feel', 'since', 'That', \"I've\", 'work', 'thing', 'take', 'before', 'things', 'new', 'while', 'probably', \"you're\", 'am', 'many', 'its', 'years', 'love', 'around', 'game', 'made', 'said', \"didn't\", '2', 'sure', 'right', 'our', 'best', 'getting', \"that's\", \"can't\", 'Not', \"doesn't\", 'We', 'does', 'down', 'few', 'find', 'used', 'day', 'He', 'bad', 'What', 'enough', 'without', 'long', 'me.', 'ever', 'doing', 'look', 'thought', 'two', 'give', 'life', 'well', 'having', 'might', 'makes', 'In', 'different', 'little', 'anything', 'through', 'it,', 'these', 'already', 'try', 'both', 'When', 'put', \"I'd\", 'character', 'shit', 'mean', 'last', 'Just', 'great', 'own', 'characters', 'us', 'trying', 'until', 'another', '3', 'No', 'least', 'went', 'keep', 'There', 'point', 'person', 'old', \"isn't\", 'here', 'She', 'big', '&gt;', 'using', 'It’s', 'hard', 'that.', 'come', 'play', 'next', 'everyone', 'For', 'able', \"That's\", 'them.', 'end', 'guy', 'bit', '5', 'world', 'tell', 'kind', 'money', 'part', 'help', 'whole', 'maybe', 'everything', 'remember', 'As', 'show', 'How', 'lol', 'time.', 'high', 'once', 'year', 'damage', 'less', 'live', 'though', '4', 'seen', 'each', 'nothing', 'told', 'may', 'stuff', 'start', 'team', 'fucking', 'saying', 'friends', \"I'll\", 'started', 'gonna', 'literally', 'main', 'making', 'real', 'away', 'reason', 'far', 'guess', 'anyone', 'such', \"they're\", '1', 'looking', 'wanted', 'I’ve', 'definitely', 'watch', \"there's\", 'came', 'believe', 'between', 'gets', 'read', 'friend', 'talking', 'almost', 'man', 'you.', 'times', 'let', 'myself', 'care', 'school', 'Is', 'nice', 'else', 'understand', 'story', 'Also', 'seems', 'dont', 'found', 'done', 'saw', 'level', 'either', 'second', 'Oh', 'full', 'change', 'didn’t', 'set', 'buy', 'fuck', 'me,', 'Why', 'hate', 'place', 'instead', 'looks', 'kids', 'hope', 'called', 'anime', 'post', 'Then', 'run', 'hit', 'free', 'name', 'fun', \"wouldn't\", 'All', 'heard', 'too.', '10', 'left', 'idea', 'One', 'stop', 'Or', 'you’re', 'took', 'worth', 'usually', 'family', 'playing', 'job', 'ask', 'movie', 'during', \"wasn't\", 'call', 'single', 'quite', 'tried', 'Yeah', 'can’t', 'that’s', 'girl', 'home', 'pay', 'comes', 'top', 'kinda', 'banner', 'basically', 'Do', 'means', 'wrong', 'Thank', 'small', 'support', 'super', 'days', 'question', 'Because', 'again', 'talk', 'out.', 'Like', \"won't\", 'build', 'now.', 'water', 'up.', 'Maybe', 'Well', 'fact', 'People', 'that,', 'against', 'rather', 'thinking', 'At', 'wish', 'half', 'car', 'though.', 'problem', 'mind', 'women', 'ones', 'games', 'working', 'cause', 'under', '*', 'house', 'couple', 'especially', 'this.', 'To', 'entire', 'sex', 'side', \"Don't\", 'completely', 'food', \"he's\", 'goes', 'asked', 'likely', 'close', 'pull', 'Now', 'mom', 'I’d', 'later', 'comment', 'matter', 'watching', 'weird', 'doesn’t', 'parents', \"aren't\", 'absolutely', 'there.', 'felt', 'Even', 'u', 'video', 'hear', '&amp;', 'Your', 'happened', 'amount', 'hours', 'kid', \"she's\", 'star', 'sounds', 'wait', 'Some', 'one.', 'knew', 'eat', 'happy', 'seem', ':)', 'others', 'guys', 'well.', 'leave', 'months', 'often', 'open', 'cool', 'head', 'kill', 'yet', 'country', 'works', 'case', 'taking', 'needs', '+', 'coming', 'power', 'you,', 'says', 'based', 'im', \"haven't\", 'sense', 'become', 'whatever', 'day.', 'exactly', 'lost', 'rest', 'sometimes', 'similar', 'Zhongli', 'crit', 'due', 'night', 'must', 'lol.', 'him.', 'dps', 'time,', 'weapon', 'enjoy', 'Yeah,', 'experience', 'easy', 'Thanks', \"There's\", 'agree', 'spend', 'gave', 'human', 'body', 'her.', 'certain', 'turn', 'men', 'etc.', 'answer', 'check', 'normal', 'ago', 'unless', 'dad', 'yourself', '6', 'fine', 'life.', 'move', 'That’s', 'favorite', 'music', 'ass', 'song', 'Most', 'Good', 'huge', 'burst', 'actual', 'seeing', 'week', 'space', 'attack', 'die', 'running', 'all.', 'takes', 'black', 'watched', 'them,', 'worked', 'add', 'past', 'again.', 'woman', ',', 'Also,', 'save', 'living', \"couldn't\", 'room', 'Its', 'deal', 'outside', 'people.', 'energy', 'type', 'per', 'played', 'system', 'low', 'content', 'phone', 'number', 'current', 'true', 'face', 'possible', 'feels', 'Yes', '.', 'good.', 'episode', '20', 'gives', 'chance', 'here.', 'behind', 'straight', 'looked', 'mostly', 'I’ll', 'event', 'early', 'feeling', 'wife', 'isn’t', 'three', 'stay', 'learn', 'amazing', 'hot', 'is.', 'sound', 'shield', 'physical', 'After', \"you'll\", 'happen', 'yeah', 'course', 'front', 'way.', 'minutes', 'middle', 'asking', 'fight', 'extra', 'thank', 'important', 'original', 'shows', '?', 'imagine', 'sleep', 'stupid', 'hell', 'sort', 'Are', 'finally', 'higher', 'damn', 'series', 'needed', 'hand', 'artifacts', 'together', '100%', 'honestly', 'on.', 'given', 'wants', 'ended', 'random', \"You're\", 'worst', 'wanna', 'child', 'On', 'break', 'social', 'Yes,', 'DPS', 'Probably', 'specific', 'US', 'worse', 'Can', 'interesting', 'game.', 'thing.', 'do.', 'giving', 'scene', 'bring', 'issue', 'near', 'turned', 'rate', 'they’re', 'thanks', 'meant', 'clear', 'bunch', 'line', 'pick', 'now,', 'is,', 'multiple', 'death', 'dead', 'up,', 'order', 'years.', 'supposed', 'decided', 'girls', 'Being', 'common', 'word', 'god', 'abyss', 'With', 'version', 'future', 'it?', 'simply', 'strong', 'season', 'Well,', 'please', 'yes', 'sorry', 'longer', 'though,', '(and', 'large', 'white', \"we're\", 'Have', 'light', 'weeks', 'difference', 'age', 'Same', 'easier', 'reading', 'issues', 'fast', 'loved', 'Never', 'along', 'work.', 'account', 'Hu', '8', 'dog', 'company', 'young', 'cryo', 'No,', 'this,', 'pyro', 'piece', 'lose', 'Which', 'totally', 'cut', '(I', 'building', 'happens', 'alone', 'older', 'not.', 'electro', 'general', 'except', 'telling', '2.', 'realize', 'Venti', 'short', '/', 'easily', 'kept', 'decent', 'wasn’t', 'group', 'funny', 'hold', 'walk', '30', 'towards', 'Im', 'Did', 'Ganyu', 'better.', 'extremely', 'out,', 'currently', 'there’s', 'Eula', 'dude', 'American', 'Any', 'bought', 'consider', '&amp;#x200B;', 'glad', 'spent', 'coffee', 'Only', 'Bennett', 'knows', 'control', 'quality', 'waiting', 'late', 'soon', 'class', 'personal', 'Every', 'within', 'mine', 'met', 'several', 'allowed', 'God', 'Lol', 'moment', 'liked', 'cannot', 'inside', 'month', 'themselves', 'standard', 'crazy', 'forget', '7', 'thats', 'party', 'relationship', 'said,', '\"I', 'perfect', 'dmg', 'wouldn’t', 'across', 'health', 'drink', 'brother', 'list', 'built', 'sad', 'expect', 'example', 'recommend', 'voice', 'to.', 'book', 'self', 'stuck', 'wonder', 'taste', 'pain', 'stopped', 'choose', 'drop', 'beat', 'mean,', 'state', 'personally', 'floor', 'in.', '12', 'paid', 'sub', 'cost', 'compared', 'pity', 'Was', 'stand', 'Please', 'assume', 'died', 'depends', 'poor', '=', 'right?', 'fan', 'brain', 'whether', 'Of', 'hour', 'Diluc', 'listen', 'weapons', 'Does', 'mental', 'off.', 'explain', 'players', 'there,', 'prefer', 'lots', 'eating', 'gay', 'killed', 'children', 'mother', 'eventually', 'know,', 'hair', 'E', 'Who', '1.', \"you've\", 'questions', 'area', 'figure', 'Genshin', 'His', 'fucked', 'well,', 'bed', 'known', 'shot', 'above', 'date', 'Jean', 'too,', 'door', 'gotten', 'day,', 'much.', 'point.', 'drive', 'cant', 'Klee', 'Diona', 'things.', 'generally', 'eyes', 'artifact', 'shit.', 'government', 'fall', 'enemies', 'problems', 'following', 'opinion', 'lower', 'form', 'Those', 'boss', 'considered', 'Reddit', 'red', 'skill', 'yes,', 'Go', 'public', 'learned', 'movies', 'popular', 'pulled', 'you!', 'ok', 'OP', 'mention', 'changed', 'art', 'war', 'he’s', 'recently', 'fit', 'luck', 'college', 'hurt', 'people,', 'wear', 'lmao', 'resin', 'tho', 'miss', 'sister', 'gone', 'stars', '15', 'New', 'fully', 'average', 'walking', 'taken', 'back.', 'more.', 'one,', 'interested', ':(']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca04419-5d1c-4c53-970b-fd62641a37ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_rng import TextRng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038825a-fffb-4c83-af95-cebca9aa56eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comments.sort_values(by=['score'], inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f325e49-b581-4858-a875-8d0f3b444fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocena bitov/s\n",
    "TR = TextRng(text=df_comments['body'].values[:10**6], \n",
    "             utf8_kwargs={\"utf8_bit_pos\": -1, \"top_words\": top_1000_words[:100], \"remove_spaces\": True},\n",
    "             mixing_kwargs = {\"n_mixes\": 1, \"chunks\": 16},\n",
    "             lognormal_kwargs = {\"n\": 8, \"d\": 6, \"div\": 1e6},\n",
    "             bit_generation=\"bitstring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df331d7-33c4-41ae-8e37-d15ffba6e72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benford_helper_functions import str_to_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4eb806-726d-4f17-9a0d-672dc7faba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bits = str_to_bits(TR.text, remove_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e753b-8fb6-41f3-95d6-3cf07689eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'{len(all_bits):.3e}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500346ec-da33-4de7-9130-c4c374907b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits_per_comment = len(all_bits) / 10**6\n",
    "bits_per_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c84948f-daaa-457e-9a3c-f8aeae03f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = TR.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c99a9ef-154b-4e8c-b4d5-8c9a843e647f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = len(all_bits) / len(bits)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465122e1-5dec-49d8-bbab-4439a5828ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(bits_per_comment / r) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e6537a-8d91-44dd-b310-1f5e38e6f213",
   "metadata": {
    "tags": []
   },
   "source": [
    "## testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b4edad-ea9c-42c3-b0af-35666a905ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bit_chunk(bits, n):\n",
    "    m = len(bits) // n\n",
    "    bits_chunked = [bits[i*m:(i+1)*m] for i in range(n)]\n",
    "    return bits_chunked\n",
    "\n",
    "\n",
    "def make_bit_chunks(bits, n=32, splits=2, prnt=False):\n",
    "    end_parts, elements = n**(splits + 1), len(bits) // n**(splits + 1)\n",
    "    if prnt:\n",
    "        print(f'end parts: {end_parts} with {elements} elements')\n",
    "    bits_chunked = make_bit_chunk(bits, n)\n",
    "    \n",
    "    if splits == 0:\n",
    "        return bits_chunked, end_parts, elements\n",
    "\n",
    "    for split in range(splits):\n",
    "        split_chunks = []\n",
    "        for chunk in bits_chunked:\n",
    "            split_chunks += make_bit_chunk(chunk, n)\n",
    "        bits_chunked = split_chunks\n",
    "    \n",
    "    return bits_chunked, end_parts, elements\n",
    "\n",
    "\n",
    "def make_bitstring_from_chunks(bits, num_bits=None, **kwargs):\n",
    "    bits_chunked, n_chunks, elements = make_bit_chunks(bits, **kwargs)\n",
    "    \n",
    "    bitstring = ''\n",
    "    for i in range(elements):\n",
    "        for j in range(n_chunks):\n",
    "            b = bits_chunked[j][i]\n",
    "            bitstring += b\n",
    "            if num_bits:\n",
    "                if len(bitstring) > num_bits:\n",
    "                    return bitstring\n",
    "        \n",
    "    return bitstring\n",
    "\n",
    "\n",
    "def multi_mix(st, n_mixes=None, chunks=None):\n",
    "    starting_st = st\n",
    "    \n",
    "    if chunks is None:\n",
    "        n = int(np.sqrt(len(st))) - 1\n",
    "    else:\n",
    "        n = chunks\n",
    "    print(f'chunks: {n}')\n",
    "    \n",
    "    if n_mixes is None:\n",
    "        n_mixes = n\n",
    "        \n",
    "    for i in tqdm(range(n_mixes)):\n",
    "        st = make_bitstring_from_chunks(st, n=n, splits=0)\n",
    "        if st == starting_st:\n",
    "            print('sequence repeated! returnig last good combination!')\n",
    "            return old_st\n",
    "        old_st = st\n",
    "    \n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96240fd2-2081-4c0c-85f3-a61e06279c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utf8_bits(text, utf8_bit_pos=-1, n_top_word_replace=100):\n",
    "    full_text = ''.join(text)\n",
    "    spaces_bits = str_to_bits(full_text, to_replace=top_1000_words[:n_top_word_replace], remove_spaces=True)\n",
    "    \n",
    "    list_bits = list(spaces_bits.split(\" \"))\n",
    "\n",
    "    bits = ''\n",
    "    for b in list_bits:\n",
    "        try:\n",
    "            bits += b[utf8_bit_pos]\n",
    "        except Exception as e:\n",
    "            print(e, b)\n",
    "            pass\n",
    "    return bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee2068-0e9d-4619-bec3-6d52febbaa7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ints_with_n_bits(bits, n):\n",
    "    m = len(bits) // n\n",
    "    \n",
    "    ints = []\n",
    "    z = 0\n",
    "    for i in range(m):\n",
    "        take = bits[i*n:(i+1)*n]\n",
    "        make_int = int(take, 2)\n",
    "        if make_int != 0:\n",
    "            ints.append(make_int)\n",
    "        else:\n",
    "            z += 1\n",
    "    \n",
    "    print(f'{z} total zeros')\n",
    "    return np.array(ints)\n",
    "\n",
    "\n",
    "def reshape_and_truncate(arr, shape):\n",
    "    desired_size_factor = np.prod([n for n in shape if n != -1])\n",
    "    if -1 in shape:  # implicit array size\n",
    "        desired_size = arr.size // desired_size_factor * desired_size_factor\n",
    "    else:\n",
    "        desired_size = desired_size_factor\n",
    "    return arr.flat[:desired_size].reshape(shape)\n",
    "\n",
    "\n",
    "def text_lognormal_dist(bits, n, d, div=1):\n",
    "    \"\"\"\n",
    "    bits: str\n",
    "        Sequence of bits\n",
    "    n: int\n",
    "        Number of bits to take together in bits sequence\n",
    "    d: int\n",
    "        Number of multiplications\n",
    "    \"\"\"\n",
    "    ints = make_ints_with_n_bits(bits, n=n)\n",
    "    ints_mat = reshape_and_truncate(ints, (len(ints) // d, d))\n",
    "    ints_prod = np.prod(ints_mat / div, axis=1)\n",
    "    return ints_prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a76d4-7376-4582-a99f-6c9fc78939ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_float_chunks(fl, n):\n",
    "    m = len(fl) // n\n",
    "    bits_chunked = [fl[i*m:(i+1)*m] for i in range(n)]\n",
    "    return bits_chunked\n",
    "\n",
    "def make_floatarr_from_chunks(fl, num_fl=None, n=2): # n -> chunks\n",
    "    floats_chunked = make_float_chunks(fl, n)\n",
    "    elements = len(fl) // n\n",
    "    \n",
    "    floatarr = []\n",
    "    for i in range(elements):\n",
    "        for j in range(n):\n",
    "            f = floats_chunked[j][i]\n",
    "            floatarr.append(f)\n",
    "            \n",
    "            if num_fl and len(floatarr) > num_fl:\n",
    "                return floatarr\n",
    "        \n",
    "    return np.array(floatarr)\n",
    "\n",
    "def multi_floatarr_from_chunks(fl, n_mixes, **kwargs):\n",
    "    for m in tqdm(range(n_mixes)):\n",
    "        fl = make_floatarr_from_chunks(fl, **kwargs)\n",
    "    return np.array(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d49063c-3629-4d41-9e3b-3bb91628a521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stat_tests import chi2_test, ks_test\n",
    "\n",
    "\n",
    "def bitstring_rng_test(rng_bits, take):\n",
    "    it = len(rng_bits) // take\n",
    "    \n",
    "    if it < 1:\n",
    "        it = 1\n",
    "    \n",
    "    results = []\n",
    "    for i in range(it):\n",
    "        print(i, '/', it)\n",
    "        res = RNG_test(rng_bits[i*take:(i+1)*take])\n",
    "        results.append(res)\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def rng_all_comments_stat_tests(df, df_col, n_comments, bit_pos=-1, take=10**6, use_walk=False):\n",
    "    it = len(df[df_col]) // n_comments\n",
    "    \n",
    "    stat_results, rng_results = [], []\n",
    "    \n",
    "    for i in range(it):\n",
    "        print(i, '/', it)\n",
    "        comment_bits = utf8_bits(df_comments[df_col].values[i*n_comments:(i+1)*n_comments], utf8_bit_pos=bit_pos)\n",
    "        comment_bits = multi_mix(comment_bits, n_mixes=1, chunks=16)\n",
    "        \n",
    "        prod = text_lognormal_dist(comment_bits, n=8, d=6, div=1e6)\n",
    "        \n",
    "        u = np.log10(prod) % 1\n",
    "        chi2, ks = chi2_test(u), ks_test(u)\n",
    "        \n",
    "        # rng_bits = get_bitstring(u)\n",
    "        if use_walk:\n",
    "            rng_bits = binary_tree_walk(u).astype(str)\n",
    "        else:\n",
    "            rng_bits = get_bitstring(u)\n",
    "        \n",
    "        print(f'NUM BITS: {len(rng_bits)}')\n",
    "        rng_bits = \"\".join(rng_bits)\n",
    "        \n",
    "        res = bitstring_rng_test(rng_bits, take=take)\n",
    "        \n",
    "        stat_results.append([chi2, ks])\n",
    "        rng_results.append(res)\n",
    "    \n",
    "    return stat_results, rng_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e683ca-5413-405c-98e0-d07f6f0e14a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb6ecce-762a-4aa8-8ece-748028eec6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = rng_all_comments_stat_tests(df_comments, 'body', 10**6, bit_pos=-2)\n",
    "\n",
    "# pickle.dump(results, open(\"results_bit_m2.p\", \"wb\"))\n",
    "# results = pickle.load(open(\"results_bit_m2.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6a2494-bf63-42ba-bedf-b11580f1eb6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = rng_all_comments_stat_tests(df_comments, 'body', 10**6, bit_pos=-3)\n",
    "\n",
    "# pickle.dump(results, open(\"results_bit_m3.p\", \"wb\"))\n",
    "# results = pickle.load(open(\"results_bit_m3.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547ac616-8bc4-4d91-91b8-3360dcfd8819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = rng_all_comments_stat_tests(df_comments, 'body', 10**6, bit_pos=-4)\n",
    "\n",
    "# pickle.dump(results, open(\"results_bit_m4.p\", \"wb\"))\n",
    "# results = pickle.load(open(\"results_bit_m4.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0c489-3f8f-4038-9df2-6b3f347d4627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# results = rng_all_comments_stat_tests(df_comments, 'body', 10**6, bit_pos=4, use_walk=True)\n",
    "\n",
    "# pickle.dump(results, open(\"results_bit_m5_walk.p\", \"wb\"))\n",
    "# results = pickle.load(open(\"results_bit_m5_walk.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546ffba-5bf9-4a5c-9ce3-1f7d288fc9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = pickle.load(open(\"results_bit_m1.p\", \"rb\"))\n",
    "res2 = pickle.load(open(\"results_bit_m2.p\", \"rb\"))\n",
    "res3 = pickle.load(open(\"results_bit_m3.p\", \"rb\"))\n",
    "res4 = pickle.load(open(\"results_bit_m4.p\", \"rb\"))\n",
    "res5 = pickle.load(open(\"results_bit_m5.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9d034e-8a3b-4044-8f60-192134a34271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_p_results(results):\n",
    "    p_results = []\n",
    "\n",
    "    for r in results[1]:\n",
    "        for ri in r:\n",
    "            p_results.append(ri['p'].values.astype(np.float32))\n",
    "    \n",
    "    return np.array(p_results)\n",
    "\n",
    "\n",
    "def get_stat_results(results):\n",
    "    chi2, ks = [], []\n",
    "    chi2_crit, ks_crit = [], []\n",
    "    \n",
    "    for r in results[0]:\n",
    "        chi2.append(r[0][0][0][0])\n",
    "        chi2_crit.append(r[0][1])\n",
    "        ks.append(r[1][0][0][0])\n",
    "        ks_crit.append(r[1][1])\n",
    "    \n",
    "    return chi2, ks, chi2_crit, ks_crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57504f4b-1128-43a4-9aac-b34f2c8ff6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [res1, res2, res3, res4, res5]\n",
    "results = results[:3]\n",
    "\n",
    "p_results = [get_p_results(i) for i in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3031292-a55c-4c33-a17c-44db81040391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = set_size_decorator(plt.subplots, fraction=1.5, ratio='4:3')(4, 4)\n",
    "ax[-1, -1].set_visible(False)\n",
    "axs = ax.flatten()\n",
    "\n",
    "# crit: 24.724970311318277\n",
    "for i in range(15):\n",
    "    l = []\n",
    "    for j in range(len(p_results)):\n",
    "        p = p_results[j]\n",
    "        _, bins, _ = axs[i].hist(p[:, i], histtype='step', bins=10)\n",
    "        c2 = chi2_test(p[:, i], n_bins=len(bins))\n",
    "        l.append(f'bit {-j-1}: $\\chi^2=${c2[0][0][0]:.2f}')\n",
    "        \n",
    "    axs[i].legend(l, loc='lower left', fontsize=5)\n",
    "    axs[i].set_title(f'test {i+1}')\n",
    "\n",
    "savefig('text_rng_p_dists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdef27-db24-4c4e-96e7-47e021200248",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, res in enumerate(results):\n",
    "    chi2, ks, chi2_crit, ks_crit = get_stat_results(res)\n",
    "    print(chi2, chi2_crit)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca5b79-1dd7-484a-8852-905dcef9a9be",
   "metadata": {
    "tags": []
   },
   "source": [
    "## more testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb234e0-cd57-4241-bc80-439c02013879",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_comments = 10**6\n",
    "\n",
    "comment_bits = utf8_bits(df_comments['body'].values[:n_comments])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a018b27-5ebe-464b-abf9-7e56d6ae2931",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_bits = multi_mix(comment_bits, n_mixes=1, chunks=16)\n",
    "\n",
    "prod = text_lognormal_dist(comment_bits, n=8, d=6, div=1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7907d5-3aaf-463e-88e5-32043e6eb12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prod.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bbaf6e-138c-4e0b-86c8-8b40311541f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stat_tests import chi2_test, ks_test\n",
    "\n",
    "u = np.log10(prod) % 1\n",
    "\n",
    "# u = multi_floatarr_from_chunks(u, n_mixes=1, n=2)\n",
    "\n",
    "# for i in range(10):\n",
    "#     u = np.concatenate((u[1::2], u[::2]))\n",
    "\n",
    "chi2_test(u), ks_test(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb619c-00b3-44e2-9b19-5f776ac255f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng_bits = get_bitstring(u)\n",
    "rng_bits = \"\".join(rng_bits)\n",
    "\n",
    "# rng_bits = binary_tree_walk(u).astype(str)\n",
    "# rng_bits = \"\".join(rng_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510fdea-c323-47f1-90bd-75ae503aeb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rng_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7261d0-96f7-4b1e-8734-6b119fc87e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG_test(rng_bits[:10**6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7081f7-99d8-450f-b718-12f080accdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9530c-5a5a-4ad4-890c-cad35773f1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
